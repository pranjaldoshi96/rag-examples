# Import necessary classes and functions from various libraries
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.runnables import RunnablePassthrough
from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings, NVIDIARerank

from ingest import get_vectorstore, ingest_document, olympics_data_retriever

# Initialize an empty list to keep track of the chat history
chat_history = []


def rewrite(query: str) -> str:
    """
    Rewrite the user's query to be self-contained and clear for better document retrieval.

    Args:
        query (str): The original user query.

    Returns:
        str: The transformed, self-contained query.
    """

    print("---TRANSFORM QUERY---")

    # Define the system prompt for the AI assistant to rewrite the query
    system_prompt = """
        You are an AI assistant tasked with rewriting user questions to make them self-contained and independently answerable. Your goal is to incorporate relevant context from the conversation history into the user's current question, creating a comprehensive query that can be answered without additional context.
        Follow these guidelines:
        - Analyze the conversation history and the user's current question.
        - Identify key information from previous exchanges that is necessary to understand the current question fully.
        - Ensure the rewritten question is clear, concise, and specific.
        - Maintain the original intent and tone of the user's question.
        - Do not add any information that's not part of the original question.
        \n\n
        "Chat History: ": {chat_history}
        """

    # Define a Pydantic model to structure the output of the rewritten query
    class RewrittenQuery(BaseModel):
        rewritten_query: str = Field(
            ..., description="The re-written question that is self-contained."
        )

    # Create a chat prompt template using the defined system prompt and user input
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}"),
        ]
    )

    # Build the chat history string for the prompt
    chat = ""
    for ch in chat_history:
        chat += f"{ch[0]}: {ch[1]}\n"

    # Initialize the ChatNVIDIA model for rewriting the query
    model = ChatNVIDIA(model="meta/llama-3.1-70b-instruct")

    # Specify that the model output should follow the structure of the RewrittenQuery model
    model = model.with_structured_output(RewrittenQuery)

    # Create a chain that applies the prompt to the model
    chain = prompt | model

    # Invoke the chain to get the rewritten query
    response = chain.invoke(
        {"input": f"{query}. Answer in one sentence", "chat_history": chat}
    )

    # Output the original and rewritten queries for debugging
    print(f"Query: {query}, \nRewritten Query: {response}")

    return response.rewritten_query


def generate_response(query: str) -> str:
    """
    Generate a response to the user's query using rewritten query and context from data retrieval.

    Args:
        query (str): The original user query.

    Returns:
        str: The final response generated by the assistant.
    """

    # Rewrite the user's query to make it self-contained
    rewritten_query = rewrite(query)
    print("Rewritten query: ", rewritten_query)

    # Retrieve context relevant to the rewritten query
    context = olympics_data_retriever(rewritten_query)

    # Define the system prompt for generating a response to the query
    system_prompt = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context and conversation history to answer "
        "the question. If you don't know the answer, say that you "
        "don't know. Use three sentences maximum and keep the "
        "answer concise."
        "\n\n"
        "Context: {context}"
        "\n\n"
        "Conversation History: {chat}"
    )

    # Build the chat history string for the prompt
    chat = ""
    for ch in chat_history:
        chat += f"{ch[0]}: {ch[1]}\n"

    # Create a chat prompt template using the defined system prompt and user input
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}"),
        ]
    )

    # Initialize the ChatNVIDIA model for generating the final response
    llm = ChatNVIDIA(model="meta/llama-3.1-70b-instruct")

    # Create a chain that applies the prompt to the model and parses the output
    rag_chain = prompt | llm | StrOutputParser()

    # Invoke the chain to get the response
    result = rag_chain.invoke({"context": context, "chat": chat, "input": query})

    # Update the chat history with the user's query and the assistant's response
    chat_history.append(("user", query))
    chat_history.append(("assistant", result))

    # Output the updated chat history for debugging
    print(chat_history)

    return result
